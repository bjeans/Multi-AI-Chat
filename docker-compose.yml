services:
  llm-council:
    image: bjeans/multi-ai-chat:latest
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-council-app
    user: "1000:1000"
    ports:
      - "8000:8000"
    environment:
      # LiteLLM Proxy Configuration
      - LITELLM_PROXY_URL=${LITELLM_PROXY_URL:-http://host.docker.internal:4000}
      - LITELLM_API_KEY=${LITELLM_API_KEY}

      # Database Configuration
      - DATABASE_URL=sqlite+aiosqlite:///./data/database.db

      # CORS Configuration
      # For development: use * or http://localhost:8000
      # For production: MUST specify exact domain(s) to prevent CSRF attacks
      # Example: CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:8000}
    volumes:
      # Persist database outside container
      - ./data:/app/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - llm-council-network

networks:
  llm-council-network:
    driver: bridge
